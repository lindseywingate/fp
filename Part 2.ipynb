{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21798a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d690b47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset/Videos/data_test1.rgb\", \"rb\") as f:\n",
    "    v = np.fromfile(f, np.dtype('B'))\n",
    "#setting the number of pixels in frame\n",
    "pxct = 129600\n",
    "#setting the number of frames\n",
    "frct = (int)(v.size/(3*pxct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42ebf9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_video = []\n",
    "for frame_number in range(frct):\n",
    "        start = (frame_number) * pxct * 3\n",
    "        curr_frame = v[start:start + 3 * pxct]\n",
    "        final_frame = np.reshape(curr_frame, (480,270,3), order = 'F')\n",
    "        final_frame = np.rot90(final_frame,3)\n",
    "        final_frame = np.flip(final_frame,1)\n",
    "        \n",
    "        #searching these final frames for logos (new cell)\n",
    "        init_video.append(final_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef2dc35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMAGE RECOGNITION FOR BRANDS\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cef44917",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity\n",
    "frame_stats = np.zeros((9000, 2)).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cdf0daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save beginning and end of cuts in number of frames\n",
    "cuts_begin = []\n",
    "cuts_end = []\n",
    "#storing the advertisements to replace with\n",
    "#indexed the same way as that of cuts_begin and cuts_end\n",
    "#if not to be replaced, a blank line\n",
    "replaced_advertisements = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84a440dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 30, (480, 270))\n",
    "for i in range(frct - 1):\n",
    "    out.write(init_video[i])\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2dea319",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image negatives for training\n",
    "# for f in range(frct - 1):\n",
    "#     if f > 3000 or f < 1200:\n",
    "#         data = Image.fromarray(init_video[f]).convert('RGB')\n",
    "#         data.save('negative_images/{}.png'.format(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93d16fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for frame_number in range(frct - 1):\n",
    "    #calculate new shots with histogram\n",
    "    #less means more similar\n",
    "    hist1 = cv2.calcHist([init_video[frame_number]],[0],None,[256],[0,256])\n",
    "    hist2 = cv2.calcHist([init_video[frame_number + 1]],[0],None,[256],[0,256])\n",
    "    score=cv2.compareHist(hist1,hist2,cv2.HISTCMP_BHATTACHARYYA)\n",
    "    \n",
    "    frame_stats[frame_number][0] = score\n",
    "    #store score for ssi\n",
    "    #less means more similar\n",
    "    #i1 = cv2.cvtColor(init_video[frame_number], cv2.COLOR_BGR2GRAY)\n",
    "    #i2 = cv2.cvtColor(init_video[frame_number + 1], cv2.COLOR_BGR2GRAY)\n",
    "    #score2 = structural_similarity(i1, i2)\n",
    "    #score2 = 1-score2 \n",
    "    #frame_stats[frame_number][1] = score2\n",
    "    \n",
    "    #score indicates sensitivity for shot changes\n",
    "    if score > 0.3:\n",
    "        #saves the image, currently used for debugging\n",
    "        #if score + score2 > 0.3:\n",
    "        data = Image.fromarray(init_video[frame_number]).convert('RGB')\n",
    "        data.save('images/{}.png'.format(frame_number))\n",
    "        data = Image.fromarray(init_video[frame_number+1]).convert('RGB')\n",
    "        data.save('images/{}.png'.format(frame_number+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ba0db14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Starbucks', 'Subway'])\n"
     ]
    }
   ],
   "source": [
    "#reading in all other advertisements\n",
    "#saves in a dictionary with first part assumed to be the company name\n",
    "advertisements = {}\n",
    "PATHS_TO_ADVERTISEMENTS = \"dataset/Ads\"\n",
    "for file in os.listdir(PATHS_TO_ADVERTISEMENTS):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith(\".rgb\"): \n",
    "        vpath = os.path.join(PATHS_TO_ADVERTISEMENTS, filename)\n",
    "        with open(vpath, \"rb\") as f:\n",
    "            v = np.fromfile(f, np.dtype('B'))\n",
    "        #setting the number of pixels in frame\n",
    "        pxct = 129600\n",
    "        #setting the number of frames\n",
    "        frct = (int)(v.size/(3*pxct))\n",
    "        video = []\n",
    "        for frame_number in range(frct):\n",
    "            start = (frame_number) * pxct * 3\n",
    "            curr_frame = v[start:start + 3 * pxct]\n",
    "            final_frame = np.reshape(curr_frame, ( 480,270, 3), order = 'F')\n",
    "            final_frame = np.rot90(final_frame,3)\n",
    "            final_frame = np.flip(final_frame,1)\n",
    "            video.append(final_frame)\n",
    "        #Get the name of the company\n",
    "        key = filename.split('_')[0]\n",
    "        advertisements[key] = video\n",
    "        \n",
    "print(advertisements.keys())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "992c1f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splicing in videos with the names \n",
    "#save beginning and end of cuts in number of frames\n",
    "cuts_begin = [2400]\n",
    "cuts_end = [2849]\n",
    "#storing the advertisements to replace with\n",
    "#indexed the same way as that of cuts_begin and cuts_end\n",
    "#if not to be replaced, an empty string\n",
    "replaced_advertisements = [\"Subway\"]\n",
    "final_video = []\n",
    "for i in range (len(cuts_begin)):\n",
    "    if i == 0:\n",
    "        final_video = init_video[:cuts_begin[i]] + advertisements[replaced_advertisements[i]] + init_video[cuts_end[i]:]\n",
    "    else:\n",
    "        #further work required to sync frames iff ad lengths are variable\n",
    "        final_video = final_video[:cuts_begin[i]] + advertisements[replaced_advertisements[i]] + final_video[cuts_end[i]:]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f386b805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73dd2398",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.5) :-1: error: (-5:Bad argument) in function 'knnMatch'\n> Overload resolution failed:\n>  - trainDescriptors is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::UMat> for argument 'trainDescriptors'\n>  - argument for DescriptorMatcher.knnMatch() given by name ('k') and position (2)\n>  - argument for DescriptorMatcher.knnMatch() given by name ('k') and position (2)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     26\u001b[0m bf \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mBFMatcher(cv2\u001b[38;5;241m.\u001b[39mNORM_HAMMING, crossCheck\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 27\u001b[0m matches \u001b[38;5;241m=\u001b[39m \u001b[43mbf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mknnMatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdes1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescriptors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(matches)\n\u001b[1;32m     30\u001b[0m good_matches \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.5.5) :-1: error: (-5:Bad argument) in function 'knnMatch'\n> Overload resolution failed:\n>  - trainDescriptors is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::UMat> for argument 'trainDescriptors'\n>  - argument for DescriptorMatcher.knnMatch() given by name ('k') and position (2)\n>  - argument for DescriptorMatcher.knnMatch() given by name ('k') and position (2)\n"
     ]
    }
   ],
   "source": [
    "#trying with orb\n",
    "#read in frame from video\n",
    "frame = Image.fromarray(final_video[2100]).convert('RGB')\n",
    "frame.save('images/box-test.png'.format(1));\n",
    "frame1 = cv2.imread('images/box-test.png')\n",
    "grayframe = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "keypoints = []\n",
    "descriptors = []\n",
    "\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "kp1, des1 = orb.detectAndCompute(grayframe,None)\n",
    "\n",
    "files = os.listdir('augmentedData/')\n",
    "#add keypoints and descriptors from all images (\"training\" descriptor)\n",
    "for f in files:\n",
    "    try:\n",
    "        logo = cv2.imread('augmentedData/'+f)\n",
    "        (kps, descs) = detector.detectAndCompute(logo, None)\n",
    "        keypoints.append(kps)\n",
    "        descriptors.append(descs)\n",
    "        #brand.append() - need to add associated brand name here\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "matches = bf.match(des1, descriptors, k=2)\n",
    "print(matches)\n",
    "\n",
    "good_matches = []\n",
    "for m,n in matches:\n",
    "    if m.distance < 0.75*n.distance:\n",
    "        good_matches.append([m])\n",
    "        \n",
    "print(good_matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e58051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #classification process\n",
    "# detector = cv2.SIFT_create()\n",
    "# keypoints = []\n",
    "# descriptors = []\n",
    "# brand = []\n",
    "\n",
    "# files = os.listdir('augmentedData/')\n",
    "# #add keypoints and descriptors from all images (\"training\" descriptor)\n",
    "# for f in files:\n",
    "#     try:\n",
    "#         logo = cv2.imread('augmentedData/'+f)\n",
    "#         (kps, descs) = detector.detectAndCompute(logo, None)\n",
    "#         keypoints.append(kps)\n",
    "#         descriptors.append(descs)\n",
    "#         #brand.append() - need to add associated brand name here\n",
    "#     except:\n",
    "#         continue\n",
    "    \n",
    "# # for f in files:\n",
    "# #     try:\n",
    "# #         logo = cv2.imread('logos/'+f)\n",
    "# #         bwLogo = cv2.cvtColor(logo, cv2.COLOR_BGR2GRAY)\n",
    "# #         (kps, descs) = detector.detectAndCompute(bwLogo, None)\n",
    "# #         keypoints.append(kps)\n",
    "# #         descriptors.append(descs)\n",
    "# #     except:\n",
    "# #         continue\n",
    "\n",
    "# # # #read in frame from video - need to do for each frame!\n",
    "# frame = Image.fromarray(final_video[2100]).convert('RGB')\n",
    "\n",
    "# # # #reformat\n",
    "# frame.save('images/box-test.png'.format(1));\n",
    "# frame1 = cv2.imread('images/box-test.png')\n",
    "\n",
    "# # # #change to grayscale\n",
    "# grayframe = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "# for d,kp in zip(descriptors, keypoints):\n",
    "#     bf = cv2.BFMatcher(cv2.NORM_L1, crossCheck=True)\n",
    "\n",
    "#     matches = bf.knnMmatch(logodescriptors,framedescriptors)\n",
    "#     matches = sorted(matches, key = lambda x:x.distance)\n",
    "#     print(\"MATCHES \", matches)\n",
    "\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561f74a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#experimenting with SIFT\n",
    "#finding points and comparing across image \n",
    "# import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "\n",
    "# #read in logo \n",
    "# logo = plt.imread('Dataset/Brand Images/subway_logo.bmp')\n",
    "# graylogo = cv2.cvtColor(logo, cv2.COLOR_BGR2GRAY)\n",
    "# #cv2.imshow('image', graylogo)\n",
    "# #cv2.waitKey(5000)\n",
    "\n",
    "# # #read in frame from video\n",
    "# frame = Image.fromarray(final_video[2100]).convert('RGB')\n",
    "\n",
    "# # #reformat\n",
    "# frame.save('images/box-test.png'.format(1));\n",
    "# frame1 = cv2.imread('images/box-test.png')\n",
    "\n",
    "# # #change to grayscale\n",
    "# grayframe = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# sift = cv2.SIFT_create()\n",
    "\n",
    "# logopoints, logodescriptors = sift.detectAndCompute(graylogo, None)\n",
    "# framepoints, framedescriptors = sift.detectAndCompute(grayframe, None)\n",
    "\n",
    "# bf = cv2.BFMatcher(cv2.NORM_L1, crossCheck=True)\n",
    "\n",
    "# matches = bf.match(logodescriptors,framedescriptors)\n",
    "# matches = sorted(matches, key = lambda x:x.distance)\n",
    "# print(\"MATCHES \", matches)\n",
    "\n",
    "# img3 = cv2.drawMatches(graylogo, logopoints, grayframe, framepoints, matches, grayframe, flags=2)\n",
    "# plt.imshow(img3),plt.show()\n",
    "\n",
    "# #find keypoints in logo\n",
    "# siftlogo = cv2.SIFT_create()\n",
    "# kp = siftlogo.detect(logo, None)\n",
    "# img = cv2.drawKeypoints(graylogo,kp,logo)\n",
    "# #plt.imshow(logo)\n",
    "    \n",
    "# # #find keypoints in image\n",
    "# siftframe = cv2.SIFT_create()\n",
    "# kp = siftframe.detect(frame1, None)\n",
    "# img = cv2.drawKeypoints(grayframe,kp,frame1)\n",
    "#plt.imshow(frame1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dcd07f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create the video\n",
    "#file path, format, frames per second, width and height\n",
    "out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 30, (480, 270))\n",
    "frames = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60]    \n",
    "\n",
    "for i in range(len(final_video)):\n",
    "    #cv2.imshow('images/box-1.png', img)\n",
    "    data = Image.fromarray(final_video[i]).convert('RGB')\n",
    "#print(type(data))\n",
    "\n",
    "#convert rgb frame to png for writing\n",
    "    data.save('images/box-test.png'.format(1));\n",
    "\n",
    "#get new saved png frame\n",
    "    img = cv2.imread('images/box-test.png')\n",
    "\n",
    "    if(i in frames):\n",
    "#draw rectangle on frame\n",
    "        cv2.rectangle(img, (10,10), (100, 100), (0, 255, 0), 2)\n",
    "#just to verify box comes out in right spot\n",
    "        #cv2.imwrite(\"images/test-box.png\", img)\n",
    "\n",
    "#change back to rgb for avi video\n",
    "        #img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        out.write(img)\n",
    "    else:\n",
    "        out.write(img)\n",
    "\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d97c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(img[0][0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5f355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create more images to test on\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "#basic code taken from https://github.com/synckey/tensorflow_lstm_ctc_ocr/blob/master/gen_no_plate_shape_version.py\n",
    "# \"\"\"\n",
    "# Created on Sat Dec 24 09:37:01 2016\n",
    "# @author: shubham\n",
    "# \"\"\"\n",
    "# import random\n",
    "# import cv2\n",
    "# import os\n",
    "# import numpy\n",
    "# import math\n",
    "# num_images = 2700\n",
    "\n",
    "# def euler_to_mat(yaw, pitch, roll):\n",
    "#     # Rotate clockwise about the Y-axis\n",
    "#     c, s = math.cos(yaw), math.sin(yaw)\n",
    "#     M = numpy.matrix([[  c, 0.,  s],\n",
    "#                       [ 0., 1., 0.],\n",
    "#                       [ -s, 0.,  c]])\n",
    "\n",
    "#     # Rotate clockwise about the X-axis\n",
    "#     c, s = math.cos(pitch), math.sin(pitch)\n",
    "#     M = numpy.matrix([[ 1., 0., 0.],\n",
    "#                       [ 0.,  c, -s],\n",
    "#                       [ 0.,  s,  c]]) * M\n",
    "\n",
    "#     # Rotate clockwise about the Z-axis\n",
    "#     c, s = math.cos(roll), math.sin(roll)\n",
    "#     M = numpy.matrix([[  c, -s, 0.],\n",
    "#                       [  s,  c, 0.],\n",
    "#                       [ 0., 0., 1.]]) * M\n",
    "\n",
    "#     return M\n",
    "\n",
    "# def make_affine_transform(from_shape, to_shape, \n",
    "#                           min_scale, max_scale,\n",
    "#                           scale_variation=1.0,\n",
    "#                           rotation_variation=1.0,\n",
    "#                           translation_variation=1.0):\n",
    "#     out_of_bounds = False\n",
    "\n",
    "#     from_size = numpy.array([[from_shape[1], from_shape[0]]]).T\n",
    "#     to_size = numpy.array([[to_shape[1], to_shape[0]]]).T\n",
    "\n",
    "#     scale = random.uniform((min_scale + max_scale) * 0.5 -\n",
    "#                            (max_scale - min_scale) * 0.5 * scale_variation,\n",
    "#                            (min_scale + max_scale) * 0.5 +\n",
    "#                            (max_scale - min_scale) * 0.5 * scale_variation)\n",
    "#     if scale > max_scale or scale < min_scale:\n",
    "#         out_of_bounds = True\n",
    "#     roll = random.uniform(-0.3, 0.3) * rotation_variation\n",
    "#     pitch = random.uniform(-0.2, 0.2) * rotation_variation\n",
    "#     yaw = random.uniform(-1.2, 1.2) * rotation_variation\n",
    "\n",
    "#     # Compute a bounding box on the skewed input image (`from_shape`).\n",
    "#     M = euler_to_mat(yaw, pitch, roll)[:2, :2]\n",
    "#     h, w = from_shape\n",
    "#     corners = numpy.matrix([[-w, +w, -w, +w],\n",
    "#                             [-h, -h, +h, +h]]) * 0.5\n",
    "#     skewed_size = numpy.array(numpy.max(M * corners, axis=1) -\n",
    "#                               numpy.min(M * corners, axis=1))\n",
    "\n",
    "#     scale *= numpy.min(to_size / skewed_size)\n",
    "\n",
    "#     trans = (numpy.random.random((2,1)) - 0.5) * translation_variation\n",
    "#     trans = ((2.0 * trans) ** 5.0) / 2.0\n",
    "#     if numpy.any(trans < -0.5) or numpy.any(trans > 0.5):\n",
    "#         out_of_bounds = True\n",
    "#     trans = (to_size - skewed_size * scale) * trans\n",
    "\n",
    "#     center_to = to_size / 2.\n",
    "#     center_from = from_size / 2.\n",
    "\n",
    "#     M = euler_to_mat(yaw, pitch, roll)[:2, :2]\n",
    "#     M *= scale\n",
    "#     M = numpy.hstack([M, trans + center_to - M * center_from])\n",
    "\n",
    "#     return M, out_of_bounds\n",
    "\n",
    "\n",
    "# for i in range(0,num_images):\n",
    "#     f=random.choice(os.listdir('logos/'))\n",
    "#     logoI = plt.imread('logos/'+f)\n",
    "#     logo = cv2.cvtColor(logoI, cv2.COLOR_BGR2GRAY)   \n",
    "    \n",
    "#     scale = 1;#random.randint(0,5)\n",
    "#     if scale!=0 and ((logo.shape[0]/scale)>10):  \n",
    "#         M, out_of_bounds = make_affine_transform(\n",
    "#                             from_shape=logo.shape,\n",
    "#                             to_shape=logo.shape,\n",
    "#                             min_scale=1,\n",
    "#                             max_scale=1,\n",
    "#                             rotation_variation=1.0,\n",
    "#                             scale_variation=1.5,\n",
    "#                             translation_variation=1.2)   \n",
    "                            \n",
    "#         im = cv2.warpAffine(logo,M,(500, 255))\n",
    "#                             #logo.shape,255)\n",
    "\n",
    "#         cv2.imwrite('augmentedData/'+str(i)+'.png', im)\n",
    "#     else:\n",
    "#         cv2.imwrite('augmentedData/'+str(i)+'.png', im)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
